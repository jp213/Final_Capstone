{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Capstone: Revisiting the Netflix Prize\n",
    "\n",
    "## Notebook 6: Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Expectations\n",
    "\n",
    "Very early in the Netflix Contest, a brilliant young computer scientist who goes by the alias 'Simon Funk' openly revealed information which brought him a great deal of notoriety (and many job offers). He revealed here: https://sifter.org/~simon/journal/20061211.html some groundbreaking information on how Singular Value Decomposition can be utilized to predict the missing ratings of a sparse User-Item matrix. This method is commonly regarded as the most powerful prediction tool for this type of data. Unfortunately, the research required to incorporate this method would consume a much greater amount of time than is allotted for this capstone. While expect to build models that improve on baseline results, I do not expect to produce an RMSE that would land atop the leaderboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import yeojohnson as yj\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso\n",
    "from sklearn.linear_model import Ridge, OrthogonalMatchingPursuit, Lars\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.8f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1408395 entries, 0 to 1408394\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   mov_id           1408395 non-null  int16  \n",
      " 1   cust_id          1408395 non-null  int32  \n",
      " 2   day_rated        1408395 non-null  int16  \n",
      " 3   mov_year         1408395 non-null  int16  \n",
      " 4   avg_rate_pm_pd   1408395 non-null  float32\n",
      " 5   avg_rate_pc_pd   1408395 non-null  float32\n",
      " 6   cust_day_count   1408395 non-null  int16  \n",
      " 7   cust_days_since  1408395 non-null  int16  \n",
      " 8   mov_days_since   1408395 non-null  int16  \n",
      " 9   mov_avg_rating   1408395 non-null  float32\n",
      " 10  cust_avg_rating  1408395 non-null  float32\n",
      " 11  mov_day_avg      1408395 non-null  float32\n",
      " 12  cust_day_avg     1408395 non-null  float32\n",
      " 13  avg_rate_yr      1408395 non-null  float32\n",
      " 14  avg_rate_cst_yr  1408395 non-null  float32\n",
      " 15  bline_approx     1408395 non-null  float32\n",
      " 16  quiz_pc          1408395 non-null  float32\n",
      "dtypes: float32(10), int16(6), int32(1)\n",
      "memory usage: 75.2 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99072112 entries, 0 to 99072111\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   tday_trans     float64\n",
      " 1   ttan_mov_year  float32\n",
      " 2   tbit_tran      int16  \n",
      " 3   tyr_trans      float64\n",
      " 4   trpcpd_cbrt    float32\n",
      " 5   tcdc_3rt       float32\n",
      " 6   tary_3rt       float32\n",
      " 7   tyj_pc         float32\n",
      "dtypes: float32(5), float64(2), int16(1)\n",
      "memory usage: 3.5 GB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1408395 entries, 0 to 1408394\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count    Dtype  \n",
      "---  ------         --------------    -----  \n",
      " 0   qday_trans     1408395 non-null  float64\n",
      " 1   qtan_mov_year  1408395 non-null  float32\n",
      " 2   qbit_tran      1408395 non-null  int16  \n",
      " 3   qyr_trans      1408395 non-null  float64\n",
      " 4   qrpcpd_cbrt    1408395 non-null  float32\n",
      " 5   qcdc_3rt       1408395 non-null  float32\n",
      " 6   qary_3rt       1408395 non-null  float32\n",
      " 7   qyj_pc         1408395 non-null  float32\n",
      "dtypes: float32(5), float64(2), int16(1)\n",
      "memory usage: 51.0 MB\n",
      "Wall time: 6.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import main training and quiz data\n",
    "base_path = 'C:/Users/jnpol/Documents/DS/Data Science/UL/'\n",
    "train_pca = pd.read_parquet(base_path + 'train_pca.parquet')\n",
    "train_trans = pd.read_parquet(base_path + 'train_trans.parquet')\n",
    "train_target = pd.read_parquet(base_path + 'train_target.parquet')\n",
    "quiz_pca = pd.read_parquet(base_path + 'quiz_pca.parquet')\n",
    "quiz_trans = pd.read_parquet(base_path + 'quiz_trans.parquet')\n",
    "quiz_target = pd.read_parquet(base_path + 'quiz_target.parquet')\n",
    "\n",
    "quiz_pca.info()\n",
    "train_trans.info()\n",
    "quiz_trans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Matters\n",
    "\n",
    "Time is crucial in all walks of life. With such a massive volume of data, it is quite reasonable to model on fractions of the dataset instead of the entire 100,000,000+ observations. I have done model runs across the spectrum of fractions, and there is very little difference (usually at the third decimal place). Perhaps if the prize was still up for grabs, I may reconsider this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_pca = train_pca.sample(frac=0.8, random_state=11)\n",
    "train_trans = train_trans.sample(frac=0.8, random_state=11)\n",
    "train_target = train_target.sample(frac=0.8, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 646 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_pca = train_pca[['cust_id', 'avg_rate_pm_pd', 'mov_avg_rating', 'cust_avg_rating']]\n",
    "quiz_pca = quiz_pca[['cust_id', 'avg_rate_pm_pd', 'mov_avg_rating', 'cust_avg_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 817 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmix = pd.concat([train_pca, train_trans.tyr_trans, train_trans.tary_3rt], axis=1)\n",
    "qmix = pd.concat([quiz_pca, quiz_trans.qyr_trans, quiz_trans.qary_3rt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training set\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(tmix)\n",
    "y_train = train_target.rating.to_numpy()\n",
    "\n",
    "# quiz set\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(qmix)\n",
    "y_test = quiz_target.rating.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algos\n",
    "rid = Ridge(max_iter=10, random_state=413)\n",
    "sgdr = SGDRegressor(alpha=5, max_iter=10, shuffle=False, verbose=1, random_state=761)\n",
    "hgbr = HistGradientBoostingRegressor(learning_rate=0.2, max_iter=250, max_leaf_nodes=200, random_state=119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge Regression was the top performer among the generalized linear models in terms of both score and computational efficiency. Utilized but discarded due to inferior results were: LinearRegression, OrthogonalMatchingPursuit, Lars, HuberRegressor, and TweedieRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.2234926118452003\n",
      "RMSE = 0.993489507504251\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridg = rid.fit(X_train, y_train)\n",
    "y_pred = ridg.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print('R-squared =', ridg.score(X_test, y_test))\n",
    "print('RMSE =', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The results from this model were difficult to make sense of.  When certain combinations of features were selected, the Ridge results would improve, and this SGDRegressor would produce worse scores.  Other combinations would produce the opposite result.  Ultimately, a StackingRegressor might produce a blended result that is superior to these algorithms taken individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.10, NNZs: 6, Bias: 3.606274, T: 79257690, Avg. loss: 0.533659\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.10, NNZs: 6, Bias: 3.604844, T: 158515380, Avg. loss: 0.533619\n",
      "Total training time: 10.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.10, NNZs: 6, Bias: 3.604104, T: 237773070, Avg. loss: 0.533612\n",
      "Total training time: 15.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.10, NNZs: 6, Bias: 3.603627, T: 317030760, Avg. loss: 0.533607\n",
      "Total training time: 20.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.10, NNZs: 6, Bias: 3.603286, T: 396288450, Avg. loss: 0.533605\n",
      "Total training time: 25.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.10, NNZs: 6, Bias: 3.603026, T: 475546140, Avg. loss: 0.533602\n",
      "Total training time: 29.96 seconds.\n",
      "Convergence after 6 epochs took 29.96 seconds\n",
      "R-squared = 0.07453113413308265\n",
      "RMSE = 1.084604483013789\n"
     ]
    }
   ],
   "source": [
    "sgd = sgdr.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print('R-squared =', sgd.score(X_test, y_test))\n",
    "print('RMSE =', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistogramGradientBoostingRegressor\n",
    "\n",
    "This model was quite a discovery. It is available via sklearn, but declared as being 'experimental'. The algorithm is incredibly fast compared with the standard GradientBoostingRegressor, and the hyperparameters are quite intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.22423733620616892\n",
      "RMSE = 0.9930129806011876\n",
      "Wall time: 16min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hgb = hgbr.fit(X_train, y_train)\n",
    "y_pred = hgb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print('R-squared =', hgb.score(X_test, y_test))\n",
    "print('RMSE =', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In terms of a best model, the HistogramGradientBoostingRegressor produced the best scores, while most of the GLM's were just slightly behind.  KNN Regression was promising in early tests, but the algorithm simply took too long to train and fit.\n",
    "\n",
    "In my estimation there are two major takeaways from this research that I hope will provide value to any data science practitioner. The first, is that datasets in general should be evaluated for the possibility of engineering new features. One should seek insights and relationships in the data that may not be obvious at first glance. This research showed a great improvement in correlation coefficients with the engineered features. I believe this approach can, should, and will be applied more frequently in the industry. Finally, the big data aspect of this project was quite revealing. Working with this dataset showed limitations both in my hardware, and even in Dask, which ironically designed for big data. I could not utilize Dask's client and workers on my machine - even using 10GB allocations for the workers. Once I needed to compute something, a warning would display, showing that 95% of the allocated memory for the workers had been exceeded, and the operation would terminate. While this was unfortunate, there were benefits from this problem. I was forced to consider every operation in terms of computational efficiency. There are several ways to process lists, series, arrays, rows, and columns. I quickly learned that Numpy set operations were extremely more powerful to conditionally select data in lists or arrays than using for or while loops. Additionally, iterating rows of a DataFrame and performing conditional operations is a dreadfully slow process. The point here, is that each operation has an associated computational cost. With small to medium datasets, we may not notice the difference. But with big data, you can be certain to encounter these challenges at several stages of the workflow, if not constantly present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
