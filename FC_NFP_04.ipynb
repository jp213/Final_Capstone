{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Capstone: Revisiting the Netflix Prize\n",
    "\n",
    "## Notebook 4: Dimensionality Reduction (Unsupervised Technique)\n",
    "\n",
    "There are three specific objectives here:\n",
    "\n",
    "1. Combine highly correlated variables\n",
    "2. Reduce overall volume; minimize information loss\n",
    "3. Implement unsupervised learning technique into capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_pca(data, col_name):\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    spca = PCA(n_components=1, whiten=True, random_state=47).fit(scaled)\n",
    "    print('Explained Variance Ratio:', spca.explained_variance_ratio_)\n",
    "    spca = PCA(n_components=1, whiten=True, random_state=47).fit_transform(scaled)\n",
    "    return pd.DataFrame(spca, columns=[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1408395 entries, 0 to 1408394\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   mov_id           1408395 non-null  int16  \n",
      " 1   cust_id          1408395 non-null  int32  \n",
      " 2   day_rated        1408395 non-null  int16  \n",
      " 3   mov_year         1408395 non-null  int16  \n",
      " 4   mov_count        1408395 non-null  int32  \n",
      " 5   rated_bycust     1408395 non-null  int16  \n",
      " 6   rate_each_day    1408395 non-null  int32  \n",
      " 7   mov_day_count    1408395 non-null  uint16 \n",
      " 8   cust_day_count   1408395 non-null  int16  \n",
      " 9   cust_days_since  1408395 non-null  int16  \n",
      " 10  mov_days_since   1408395 non-null  int16  \n",
      " 11  mov_avg_rating   1408395 non-null  float32\n",
      " 12  cust_avg_rating  1408395 non-null  float32\n",
      " 13  mov_day_avg_rl   1408395 non-null  float32\n",
      " 14  mov_day_avg      1408395 non-null  float32\n",
      " 15  cust_avg_offset  1408395 non-null  float32\n",
      " 16  cust_day_avg     1408395 non-null  float32\n",
      " 17  avg_rate_mov_yr  1408395 non-null  float32\n",
      " 18  avg_rate_cst_yr  1408395 non-null  float32\n",
      " 19  global_mean      1408395 non-null  float32\n",
      " 20  cust_glob_diff   1408395 non-null  float32\n",
      " 21  mov_glob_diff    1408395 non-null  float32\n",
      "dtypes: float32(11), int16(7), int32(3), uint16(1)\n",
      "memory usage: 96.7 MB\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import data and reset_index to avoid mismatched indices when concatenating\n",
    "base_path = 'C:/Users/jnpol/Documents/DS/Data Science/UL/'\n",
    "quiz_features = pd.read_parquet(base_path + 'quiz_features.parquet')\n",
    "quiz_features.reset_index(drop=True, inplace=True)\n",
    "quiz_rows = len(quiz_features)\n",
    "quiz_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.97932469]\n",
      "Explained Variance Ratio: [0.86940819]\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpca_quiz = quiz_features[['cust_avg_offset', 'cust_glob_diff']].copy()\n",
    "cust_pc_quiz = scale_pca(cpca_quiz, 'cust_pc')\n",
    "del cpca_quiz\n",
    "\n",
    "mpca_quiz = quiz_features[['mov_day_avg_rl', 'mov_avg_rating',\n",
    "                           'mov_glob_diff']].copy()\n",
    "\n",
    "quiz_features.drop(['cust_avg_offset', 'cust_glob_diff', 'mov_day_avg_rl',\n",
    "                     'mov_avg_rating', 'mov_glob_diff'], 1, inplace=True)\n",
    "\n",
    "mov_pc_quiz = scale_pca(mpca_quiz, 'mov_pc')\n",
    "del mpca_quiz\n",
    "\n",
    "quiz_features = pd.concat([quiz_features, cust_pc_quiz, mov_pc_quiz], axis=1)\n",
    "del cust_pc_quiz, mov_pc_quiz\n",
    "quiz_features.to_parquet('quiz_pca.parquet')\n",
    "del quiz_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96304740 entries, 0 to 96304739\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   mov_id           int16  \n",
      " 1   cust_id          int32  \n",
      " 2   day_rated        int16  \n",
      " 3   mov_year         int16  \n",
      " 4   mov_count        int32  \n",
      " 5   rated_bycust     int16  \n",
      " 6   rate_each_day    int32  \n",
      " 7   mov_day_count    uint16 \n",
      " 8   cust_day_count   int16  \n",
      " 9   cust_days_since  int16  \n",
      " 10  mov_days_since   int16  \n",
      " 11  mov_avg_rating   float32\n",
      " 12  cust_avg_rating  float32\n",
      " 13  mov_day_avg_rl   float32\n",
      " 14  mov_day_avg      float32\n",
      " 15  cust_avg_offset  float32\n",
      " 16  cust_day_avg     float32\n",
      " 17  avg_rate_mov_yr  float32\n",
      " 18  avg_rate_cst_yr  float32\n",
      " 19  global_mean      float32\n",
      " 20  cust_glob_diff   float32\n",
      " 21  mov_glob_diff    float32\n",
      "dtypes: float32(11), int16(7), int32(3), uint16(1)\n",
      "memory usage: 6.5 GB\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_features = pd.read_parquet(base_path + 'train_features.parquet')\n",
    "train_features.reset_index(drop=True, inplace=True)\n",
    "train_rows = len(train_features)\n",
    "train_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.94256899]\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpca_train = train_features[['cust_avg_offset', 'cust_glob_diff']].copy()\n",
    "cust_pc_train = scale_pca(cpca_train, 'cust_pc')\n",
    "del cpca_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mpca_train1 = train_features[['mov_day_avg_rl', 'mov_avg_rating', 'mov_glob_diff']].copy()\n",
    "mpca_train2 = train_features[['mov_avg_rating', 'mov_glob_diff']].copy()\n",
    "mpca_train3 = train_features[['mov_day_avg_rl', 'mov_glob_diff']].copy()\n",
    "mpca_train4 = train_features[['mov_day_avg_rl', 'mov_avg_rating']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.86009601]\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mov_pc_train1 = scale_pca(mpca_train1, 'mov_pc_all')\n",
    "del mpca_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.95526213]\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mov_pc_train2 = scale_pca(mpca_train2, 'mov_pc_no_rl')\n",
    "del mpca_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.84457002]\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mov_pc_train3 = scale_pca(mpca_train3, 'mov_pc_no_mar')\n",
    "del mpca_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.84462326]\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mov_pc_train4 = scale_pca(mpca_train4, 'mov_pc_no_mgd')\n",
    "del mpca_train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_features.drop(['cust_avg_offset', 'cust_glob_diff', 'mov_day_avg_rl',\n",
    "                     'mov_avg_rating', 'mov_glob_diff'], 1, inplace=True)\n",
    "\n",
    "mov_pcs_train = pd.concat([mov_pc_train1, mov_pc_train2,\n",
    "                           mov_pc_train3, mov_pc_train4], axis=1)\n",
    "\n",
    "mov_pcs_train.to_parquet('mov_pcs_train.parquet')\n",
    "del mov_pcs_train\n",
    "\n",
    "train_features = pd.concat([train_features, cust_pc_train], axis=1)\n",
    "del cust_pc_train\n",
    "train_features.to_parquet('train_pca.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n",
    "As with PCA, UMAP was attempted on the quiz set first, in order to estimate both processing time and memory consumption prior to processing the training set. I terminated the process after 3 minutes, since the training set would have taken a minimum of 3.5 hours to process. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set is approximately 68.38 times larger than the quiz set.\n"
     ]
    }
   ],
   "source": [
    "print('The training set is approximately',\n",
    "      round(train_rows/quiz_rows, 2), 'times larger than the quiz set.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
